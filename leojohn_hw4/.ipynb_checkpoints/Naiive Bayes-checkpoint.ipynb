{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import arff_parser\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_parser = arff_parser.ARFF_Parser()\n",
    "test_parser = arff_parser.ARFF_Parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_parser.parse(\"lymph_train.arff\")\n",
    "test_parser.parse(\"lymph_test.arff\")\n",
    "train_data = pd.DataFrame(train_parser.data)\n",
    "test_data = pd.DataFrame(test_parser.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    " \n",
    "def topsort(graph):\n",
    "\n",
    "    in_degree = { u : 0 for u in graph }     # determine in-degree \n",
    "    for u in graph:                          # of each node\n",
    "        for v in graph[u]:\n",
    "            in_degree[v] += 1\n",
    " \n",
    "    Q = deque()                 # collect nodes with zero in-degree\n",
    "    for u in in_degree:\n",
    "        if in_degree[u] == 0:\n",
    "            Q.appendleft(u)\n",
    " \n",
    "    L = []     # list for order of nodes\n",
    "     \n",
    "    while Q:                \n",
    "        u = Q.pop()          # choose node of zero in-degree\n",
    "        L.append(u)          # and 'remove' it from graph\n",
    "        for v in graph[u]:\n",
    "            in_degree[v] -= 1\n",
    "            if in_degree[v] == 0:\n",
    "                Q.appendleft(v)\n",
    " \n",
    "    if len(L) == len(graph):\n",
    "        return L\n",
    "    else:                    # if there is a cycle,  \n",
    "        return []            # then return an empty list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 3)"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import itertools\n",
    "import pandas as pd\n",
    "class NaiveBayes(object):\n",
    "    def __init__(self,train_parser,classification_type=\"n\"):\n",
    "        self.laplace_class_counts = {}\n",
    "        self.prior_probabilities={}\n",
    "        self.conditional_probabilities = {}\n",
    "        self.joint_probabilities = {}\n",
    "        self.conditional_laplacian_counts = {}\n",
    "        self.dependent_attribute =None\n",
    "        self.dependent_attribute_values = None\n",
    "        self.root_node = None\n",
    "        self.feature_attributes =None\n",
    "        self.conditional_mutual_info = defaultdict()\n",
    "        self.graph=OrderedDict()\n",
    "        self.tan_conditional_counts ={}\n",
    "        self.tan_conditional_probabilities ={}\n",
    "        self.classification_type =classification_type\n",
    "        self.train_parser = train_parser\n",
    "\n",
    "            \n",
    "    def calculate_conditional_mutual_information(self):\n",
    "        needed_columns =self.feature_attributes \n",
    "        for index_1 in xrange(0,len(needed_columns)) :\n",
    "            for index_2 in xrange(0,len(needed_columns)) :\n",
    "\n",
    "                    column_1= needed_columns[index_1]\n",
    "                    column_2= needed_columns[index_2]\n",
    "\n",
    "                    self.conditional_mutual_info[(column_1,column_2)]=0\n",
    "                    for col_val_1 in self.train_parser.attribute_val_map[column_1]:\n",
    "                        for col_val_2 in self.train_parser.attribute_val_map[column_2]:\n",
    "                            for class_val in self.train_parser.attribute_val_map[self.dependent_attribute]:    \n",
    "                                self.conditional_mutual_info[(column_1,column_2)] +=\\\n",
    "                                self.joint_probabilities[(column_1,column_2)][(col_val_1,col_val_2,class_val)]*\\\n",
    "                                np.log2(\\\n",
    "                                float(self.conditional_probabilities[(column_1,column_2)][(col_val_1,col_val_2,class_val)])/\\\n",
    "                                (self.conditional_probabilities[column_1][(col_val_1,class_val)]*self.conditional_probabilities[column_2][(col_val_2,class_val)])\n",
    "                                )\n",
    "\n",
    "                \n",
    "    def prims(self):\n",
    "        costs = [0*len(self.feature_attributes)]\n",
    "        nodes = self.feature_attributes\n",
    "        self.root_node = nodes[0]\n",
    "\n",
    "        queue=nodes[1 :]\n",
    "        #print \"***\"\n",
    "        #print queue\n",
    "        #print self.root_node\n",
    "        #print \"***\"\n",
    "        mst_edges =[]\n",
    "        mst_edge_indices =[]\n",
    "        nodes_in_mst=[self.root_node]\n",
    "        while (len(queue)>0):\n",
    "            max_wt=-1000\n",
    "            max_to_node =None\n",
    "            max_from_node = None\n",
    "            max_index =0\n",
    "            for node_1 in nodes_in_mst:\n",
    "                for index,node_2 in enumerate(queue):\n",
    "\n",
    "                    if self.conditional_mutual_info[(node_1,node_2)]>max_wt:\n",
    "                        max_wt = self.conditional_mutual_info[(node_1,node_2)]\n",
    "                        max_to_node = node_2\n",
    "                        max_from_node = node_1\n",
    "                        max_index = index\n",
    "            nodes_in_mst.append(max_to_node)\n",
    "            mst_edges.append((max_from_node,max_to_node))\n",
    "            mst_edge_indices.append((nodes.index(max_from_node),\\\n",
    "                                   nodes.index(max_to_node)))\n",
    "            \n",
    "            queue.remove(max_to_node)\n",
    "        return {\"vertices\":nodes_in_mst,\"edges\":mst_edges}\n",
    "        \n",
    "        \n",
    "\n",
    "    def calculate_tan_conditional_probabilities(self):\n",
    "        train_data = pd.DataFrame(self.train_parser.data)\n",
    "        for node in self.graph:\n",
    "            column_class_count_dict = \\\n",
    "            train_data.groupby([node]+self.graph[node]).size().to_dict()\n",
    "            column_values =[]\n",
    "            for column in [node]+self.graph[node]:\n",
    "                column_values.append(self.train_parser.attribute_val_map[column])\n",
    "            keys = list(itertools.product(*column_values))\n",
    "            for key in keys:\n",
    "                if key in column_class_count_dict:\n",
    "                    column_class_count_dict[key]+=1\n",
    "                else:\n",
    "                    column_class_count_dict[key]=1\n",
    "            final_key = tuple([node]+self.graph[node])\n",
    "            \n",
    "            self.tan_conditional_counts[final_key] = column_class_count_dict\n",
    "  \n",
    "        for node in self.graph: \n",
    "            final_key = tuple([node]+self.graph[node])\n",
    "            column_values=[]\n",
    "\n",
    "            for column in self.graph[node]:\n",
    "                column_values.append(self.train_parser.attribute_val_map[column])\n",
    "            keys = list(itertools.product(*column_values))\n",
    "\n",
    "            conditional_probability_table=defaultdict()\n",
    "            for key in keys:\n",
    "                total=0\n",
    "                \n",
    "                for val in self.train_parser.attribute_val_map[node]:\n",
    "                    new_key =(val,)+key\n",
    "                    total = total+\\\n",
    "                             self.tan_conditional_counts[final_key][new_key]\n",
    "                        \n",
    "                for val in self.train_parser.attribute_val_map[node]: \n",
    "                    new_key =(val,)+key\n",
    "                    conditional_probability_table[new_key]=\\\n",
    "                    self.tan_conditional_counts[final_key][new_key]/float(total)\n",
    "                    self.tan_conditional_probabilities[final_key]=conditional_probability_table\n",
    "            \n",
    "            \n",
    "    def fit(self):\n",
    "\n",
    "        self.dependent_attribute =self.train_parser.dependent_attribute\n",
    "        self.feature_attributes = [attr \\\n",
    "                                   for attr in self.train_parser.attributes if attr != self.dependent_attribute]\n",
    "\n",
    "        #Normal Conditional probabilities\n",
    "        self.calculate_probabilities(train_data)    \n",
    "        \n",
    "        if self.classification_type == \"t\":\n",
    "            self.calculate_conditional_mutual_information(self.train_parser)\n",
    "            result = self.prims()\n",
    "            edges = result[\"edges\"]\n",
    "            vertices = result[\"vertices\"]\n",
    "            adjacency_matrix =defaultdict(list)\n",
    "            for edge in edges:\n",
    "                adjacency_matrix[edge[0]].append(edge[1])\n",
    "            for vertex in vertices: \n",
    "                if vertex not in adjacency_matrix:\n",
    "                    adjacency_matrix[vertex]=[]\n",
    "   \n",
    "            \n",
    "            #Creating the graph of parent relationships\n",
    "            parental_graph = defaultdict(list)\n",
    "            for edge in edges:\n",
    "                parental_graph[edge[1]].append(edge[0])\n",
    "            for vertex in vertices: \n",
    "                if vertex not in parental_graph:\n",
    "                    parental_graph[vertex]=[]  \n",
    "            for node in self.feature_attributes:\n",
    "                self.graph[node] =parental_graph[node]\n",
    "                self.graph[node].append(self.dependent_attribute)\n",
    "            for node in self.graph:    \n",
    "                print \" \".join([node]+self.graph[node]) \n",
    "            self.calculate_tan_conditional_probabilities(self.train_parser)    \n",
    "            \n",
    "        else:\n",
    "            for attr in self.feature_attributes:\n",
    "                print \" \".join([attr, self.dependent_attribute])\n",
    "            \n",
    "                \n",
    "                   \n",
    "    def calculate_probabilities(self,training_data):\n",
    "        \n",
    "        \n",
    "        train_data =pd.DataFrame(training_data)\n",
    "        self.dependent_attribute = self.train_parser.dependent_attribute\n",
    "        train_sample_size = train_data.shape[0]\n",
    "        self.dependent_attribute_values = self.train_parser.attribute_val_map[self.dependent_attribute]\n",
    "        for values in train_data[self.dependent_attribute].unique():\n",
    "            self.laplace_class_counts[values] =\\\n",
    "                train_data.groupby(self.dependent_attribute).size()[values]+1\n",
    "        for values in train_data[self.dependent_attribute].unique():    \n",
    "            self.prior_probabilities[values] =\\\n",
    "                float(self.laplace_class_counts[values])/\\\n",
    "                sum(self.laplace_class_counts.values())\n",
    "\n",
    "                \n",
    "        #Conditional laplacian counts\n",
    "        for column in self.train_parser.attributes :\n",
    "            #if column !=dependent_attribute:\n",
    "                column_class_count_dict = train_data.groupby([column,self.dependent_attribute]).size().to_dict()\n",
    "                for col_val in self.train_parser.attribute_val_map[column]:\n",
    "                    for class_val in self.train_parser.attribute_val_map[self.dependent_attribute]:\n",
    "                        if (col_val,class_val) not in column_class_count_dict:\n",
    "                            column_class_count_dict [(col_val,class_val)]=1\n",
    "                        else:\n",
    "                            column_class_count_dict [(col_val,class_val)]+=1         \n",
    "\n",
    "                self.conditional_laplacian_counts[column]=column_class_count_dict\n",
    "        \n",
    "        #Conditional laplacian counts for pairs of  attributes\n",
    "        for column_1 in self.train_parser.attributes[:-1] :\n",
    "            for column_2 in self.train_parser.attributes[:-1] :\n",
    "                column_class_count_dict = train_data.groupby([column_1,column_2,self.dependent_attribute]).size().to_dict()\n",
    "                for col_val_1 in self.train_parser.attribute_val_map[column_1]:\n",
    "                    for col_val_2 in self.train_parser.attribute_val_map[column_2]:\n",
    "                        for class_val in self.train_parser.attribute_val_map[self.dependent_attribute]:\n",
    "                                if (col_val_1,col_val_2,class_val) not in column_class_count_dict:\n",
    "                                    column_class_count_dict [(col_val_1,col_val_2,class_val)]=1\n",
    "                                else:\n",
    "                                    column_class_count_dict [(col_val_1,col_val_2,class_val)]+=1         \n",
    "\n",
    "                self.conditional_laplacian_counts[(column_1,column_2)]=column_class_count_dict        \n",
    "        #Conditional probabilities for  one attribute\n",
    "        for column in self.train_parser.attributes :\n",
    "            \n",
    "                total = defaultdict()\n",
    "                for class_val in self.train_parser.attribute_val_map[self.dependent_attribute]:\n",
    "                    for col_val in self.train_parser.attribute_val_map[column]:\n",
    "                        total[class_val] = total.get(class_val,0)+\\\n",
    "                                            self.conditional_laplacian_counts[column][(col_val,class_val)]\n",
    "                conditional_probability_dict=defaultdict()\n",
    "                for class_val in self.train_parser.attribute_val_map[self.dependent_attribute]:\n",
    "                    for col_val in self.train_parser.attribute_val_map[column]:\n",
    "                        conditional_probability_dict[(col_val,class_val)] = \\\n",
    "                            float(self.conditional_laplacian_counts[column][(col_val,class_val)])/total[class_val]\n",
    "                self.conditional_probabilities[column]=conditional_probability_dict            \n",
    "                \n",
    "        #conditional probability distribution for pairs of attrinbutes\n",
    "        needed_columns = self.feature_attributes\n",
    "        for index_1 in xrange(0,len(needed_columns)) :\n",
    "            for index_2 in xrange(0,len(needed_columns)) :\n",
    "\n",
    "                    column_1= needed_columns[index_1]\n",
    "                    column_2= needed_columns[index_2]\n",
    "                    total= defaultdict()\n",
    "                    for class_val in self.train_parser.attribute_val_map[self.dependent_attribute]:\n",
    "                        total[class_val]=0\n",
    "                        for col_val_1 in self.train_parser.attribute_val_map[column_1]:\n",
    "                            for col_val_2 in self.train_parser.attribute_val_map[column_2]:\n",
    "                                \n",
    "                                total[class_val] = total.get(class_val,0)+\\\n",
    "                                                self.conditional_laplacian_counts[(column_1,column_2)][(col_val_1,col_val_2,class_val)] \n",
    "                    \n",
    "                    conditional_probability_dict=defaultdict()\n",
    "                    for class_val in self.train_parser.attribute_val_map[self.dependent_attribute]:\n",
    "                        for col_val_1 in self.train_parser.attribute_val_map[column_1]:\n",
    "                            for col_val_2 in self.train_parser.attribute_val_map[column_2]:\n",
    "                                conditional_probability_dict[(col_val_1,col_val_2,class_val)] = \\\n",
    "                                    float(self.conditional_laplacian_counts[(column_1,column_2)][(col_val_1,col_val_2,class_val)])/total[class_val]\n",
    "                    self.conditional_probabilities[(column_1,column_2)]= conditional_probability_dict  \n",
    "\n",
    "                \n",
    "        #joint probability distribution for pairs of attrinbutes\n",
    "\n",
    "        needed_columns = self.feature_attributes\n",
    "        for index_1 in xrange(0,len(needed_columns)) :\n",
    "            for index_2 in xrange(0,len(needed_columns)) :\n",
    "\n",
    "                    column_1= needed_columns[index_1]\n",
    "                    column_2= needed_columns[index_2]\n",
    "                    total = train_data.shape[0]+\\\n",
    "                    (len(self.train_parser.attribute_val_map[column_1])*\\\n",
    "                     len(self.train_parser.attribute_val_map[column_2])*\\\n",
    "                     len(self.train_parser.attribute_val_map[self.dependent_attribute])\n",
    "                    )\n",
    "\n",
    "                    joint_probability_dict=defaultdict()\n",
    "                    for class_val in self.train_parser.attribute_val_map[self.dependent_attribute]:\n",
    "                        for col_val_1 in self.train_parser.attribute_val_map[column_1]:\n",
    "                            for col_val_2 in self.train_parser.attribute_val_map[column_2]:\n",
    "                                joint_probability_dict[(col_val_1,col_val_2,class_val)] = \\\n",
    "                                    float(self.conditional_laplacian_counts[(column_1,column_2)][(col_val_1,col_val_2,class_val)])/total\n",
    "                    self.joint_probabilities[(column_1,column_2)]= joint_probability_dict  \n",
    "        \n",
    "    def predict(self,test_data):\n",
    "        print \"\"\n",
    "        correctly_classified_instances = 0        \n",
    "        if self.classification_type =='n':\n",
    "            for data in test_data:\n",
    "                final_score=defaultdict()\n",
    "\n",
    "                for class_val in self.dependent_attribute_values:\n",
    "                    final_score[class_val] = self.prior_probabilities[class_val]\n",
    "                    for index in xrange(0,len(self.feature_attributes)):\n",
    "                        attr_name = self.feature_attributes[index]\n",
    "                        attr_value = data[index]\n",
    "                        final_score[class_val] = final_score[class_val]*\\\n",
    "                                                 self.conditional_probabilities[attr_name][(attr_value,class_val)]\n",
    "                max_score = None             \n",
    "                final_class = None\n",
    "                classes = final_score.keys()\n",
    "                scores = final_score.values()\n",
    "                max_score = max(scores)\n",
    "                #print scores\n",
    "                final_class = classes[scores.index(max_score)]\n",
    "                if final_class == data[-1]:\n",
    "                    correctly_classified_instances +=1\n",
    "                print \"{0} {1} {2:.12f}\".format(final_class,data[-1],float(max_score)/sum(final_score.values()))\n",
    "            print \"\\n{0}\".format(correctly_classified_instances)    \n",
    "        else:            \n",
    "            for data in test_data:\n",
    "                final_score=defaultdict()\n",
    "                for class_val in self.dependent_attribute_values:\n",
    "                    final_score[class_val] = self.prior_probabilities[class_val]\n",
    "                for node,parents in self.graph.iteritems():\n",
    "                    final_key =  tuple([node]+parents)\n",
    "                    cpt = self.tan_conditional_probabilities[final_key]\n",
    "                    key=[]\n",
    "                    #exclude the class_atrtribute\n",
    "                    for column in [node]+parents[:-1]:\n",
    "                        attr_index = self.feature_attributes.index(column)\n",
    "                        attr_value = data[attr_index]\n",
    "                        key.append(attr_value)\n",
    "\n",
    "                    for class_val in self.dependent_attribute_values:\n",
    "                        #Modify the key\n",
    "                        new_key =tuple(key)+(class_val,)\n",
    "                        final_score[class_val]=final_score[class_val]*cpt[new_key]\n",
    "                max_score = None             \n",
    "                final_class = None\n",
    "                classes = final_score.keys()\n",
    "                scores = final_score.values()\n",
    "                max_score = max(scores)\n",
    "                #print scores\n",
    "                final_class = classes[scores.index(max_score)]\n",
    "                if final_class == data[-1]:\n",
    "                    correctly_classified_instances +=1\n",
    "                print \"{0} {1} {2:.12f}\".format(final_class,data[-1],float(max_score)/sum(final_score.values()))\n",
    "            print \"{0}\\n\".format(correctly_classified_instances)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes at least 2 arguments (1 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-369-2fb060486df8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnb_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNaiveBayes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnb_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_parser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnb_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_parser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() takes at least 2 arguments (1 given)"
     ]
    }
   ],
   "source": [
    "nb_classifier = NaiveBayes(train_parser,sys.argv[3])\n",
    "nb_classifier.fit(train_data)\n",
    "nb_classifier.predict(test_parser.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29\n",
      " 30 31 32 33 34 35 36 37 38 39 40 41] [0 1 2 3 4]\n",
      "[ 0  1  2  3  4 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29\n",
      " 30 31 32 33 34 35 36 37 38 39 40 41] [5 6 7 8 9]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29 30 31 32 33 34 35 36 37 38 39 40 41] [10 11 12 13]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29 30 31 32 33 34 35 36 37 38 39 40 41] [14 15 16 17]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 22 23 24 25 26 27 28\n",
      " 29 30 31 32 33 34 35 36 37 38 39 40 41] [18 19 20 21]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 26 27 28\n",
      " 29 30 31 32 33 34 35 36 37 38 39 40 41] [22 23 24 25]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 30 31 32 33 34 35 36 37 38 39 40 41] [26 27 28 29]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 34 35 36 37 38 39 40 41] [30 31 32 33]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 38 39 40 41] [34 35 36 37]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37] [38 39 40 41]\n"
     ]
    }
   ],
   "source": [
    "#Crossvalidation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "for train_index, test_index in skf.split(test_parser.data , [1 for i in xrange(0,test_parser.data.shape[0])]):\n",
    "    train_index, test_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[42]"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-358-1d76c719783e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
